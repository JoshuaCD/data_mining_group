{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Dataset Clustering\n",
    "\n",
    "## Employ multiple clustering techniques on the Iris dataset, with and without PCA.  Evaluate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans, MeanShift\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Function for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_clustering(func, data, target, **args):\n",
    "        \n",
    "        \"\"\"This function generates a clustering model using a scikit-learn function, displays a confusion matrix and plot of\n",
    "        the data (using the first 3 columns of the data), and returns the model\n",
    "            \n",
    "            Arguments:\n",
    "            func - A scikit-learn clustering function\n",
    "            data - A pandas dataframe containing the features to be used in clustering\n",
    "            target - The target values for comparison with the model predictions\n",
    "            \n",
    "            Any keyword arguments requried for the specific clustering function should also be \n",
    "            included at the end of the call\n",
    "        \"\"\"\n",
    "        \n",
    "        normalized = normalize(data)\n",
    "        \n",
    "        model = func(**args)\n",
    "        preds = model.fit_predict(normalized)\n",
    "        \n",
    "        preds_df = data.copy()\n",
    "        preds_df['target'] = target\n",
    "        preds_df['preds'] = preds\n",
    "        \n",
    "        conf = pd.crosstab(preds_df['preds'], preds_df['target'], margins=True)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        display(conf)\n",
    "        \n",
    "        homogeneity = metrics.homogeneity_score(preds_df[\"target\"], preds_df[\"preds\"])\n",
    "        completeness = metrics.completeness_score(preds_df[\"target\"], preds_df[\"preds\"])\n",
    "        v_measure = metrics.v_measure_score(preds_df[\"target\"], preds_df[\"preds\"], beta=1.0)\n",
    "\n",
    "        print(f\"Homogeneity_score: {homogeneity:.2f}, measures that each cluster contains only members of a single class\")\n",
    "        print(f\"Completeness_score: {completeness:.2f}, measures that all members of a given class are assigned to the same cluster\")\n",
    "        print(f\"V-measure: {v_measure:.2f}, harmonic mean of homogeneity and completeness\")\n",
    "\n",
    "        scatter = px.scatter_3d(preds_df, x=preds_df.columns[0], y=preds_df.columns[1], z=preds_df.columns[2],\n",
    "                            color = 'target', symbol = 'preds', size_max=8,)\n",
    "        scatter.show()\n",
    "        \n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset, Explore and Display Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "iris_df['target'] = iris_df['target'].replace([0,1,2],['setosa', 'versicolor', 'virginica'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 3 unique target variables: setosa, versicolor and virginica; each a species of Iris\n",
    "* Changed the target attribute labels to a descriptive string vs numerical category for ease in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The dataset contains 150 observations, has 4 predictive attributes and 1 target variable\n",
    "* The 4 predictive attributes are numerical, the target variable is categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* petal length has the largest range and greatest variation of the 4 attributes, and also has the greatest difference of the 4 between its mean and median\n",
    "* for the other three attributes, their mean approximates their median which suggests the mean is not affected by outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(iris_df)\n",
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "* The dataset has zero missing observations\n",
    "* This is a balanced dataset in that each of the three target labels have the same number of observations\n",
    "* The distributions of sepal length and sepal width are fairly normal\n",
    "* The distributions of petal length and petal width both have two distinct groupings\n",
    "* Correlation - because the 4 predictive attributes are all numerical, refer to the Pearson's r chart, above:\n",
    "    * Sepal width and sepal length appear to be uncorrelated\n",
    "    * Petal width and petal length appear to be highly correlated \n",
    "    * Petal length and sepal length appear to be fairly correlated\n",
    "    * Petal width and sepal length also appear to be correlated, though less so than petal length and sepal length    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Elbow Plot to determine optimal number of clusters for KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Iris data frame without the target column\n",
    "\n",
    "iris_features = iris_df.drop(columns='target')\n",
    "iris_target = iris_df['target']\n",
    "iris_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Elbow plot of inertia values to determine optimal number of clusters to use in a K-Means clustering method\n",
    "# inertia is the sum of the squared distances of observations to their closest cluster center\n",
    "\n",
    "inertia_values = []\n",
    "cluster_centers = []\n",
    "K = range(1,11) #Try number of clusters from 1 to 10\n",
    "\n",
    "for k in K:\n",
    "    k_mean_model = KMeans(n_clusters = k)\n",
    "    k_mean_model.fit(iris_features)\n",
    "    inertia_values.append(k_mean_model.inertia_) #track the inertia values for each number of clusters\n",
    "    cluster_centers.append(k_mean_model.cluster_centers_) #track the cluster centers for each number of clusters\n",
    "\n",
    "# Create data frame of values for elbow plot\n",
    "elbow_data = {'Number of Clusters': K, 'Inertia': inertia_values}\n",
    "elbow_df = pd.DataFrame(elbow_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the Elbow plot\n",
    "\n",
    "fig_dims = (8, 5)\n",
    "fig, ax = plt.subplots(figsize=fig_dims)\n",
    "\n",
    "sns.set_theme(style = \"whitegrid\")\n",
    "sns.pointplot(data = elbow_df, x = 'Number of Clusters' ,y = 'Inertia', markers=[\"o\"])\\\n",
    ".set(title='Eblow Plot using Inertia for K-Means Clustering of Iris Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of clusters is at the \"elbow\" of the graph, where the Inertia begins to decline in a linear fashion.  In this case the optimal number is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display cluster centers for k = 3 clusters in k-means model\n",
    "cluster_centers[2:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dendogram to determine optimal number of clusters for Agglomerative method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_normalized = pd.DataFrame(normalize(iris_features),columns=iris_features.columns[:])\n",
    "iris_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like an elbow plot for k-means, a dendogram helps determine the ideal number of clusters for hierarchical clustering\n",
    "# Note: based on this chart, we'll go down to the 3rd level and use 3 clusters\n",
    "\n",
    "plt.figure(figsize=(18, 7))  \n",
    "plt.title(\"Dendrograms\")  \n",
    "x = shc.linkage(iris_normalized, method = 'ward')\n",
    "dend = shc.dendrogram(x,above_threshold_color=\"blue\", color_threshold=.25, orientation='right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above dendogram, we will use 3 clusters for our Agglomerative clustering analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce data frame using Principal Components Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA is affected by scale, so we need to scale our features before applying PCA\n",
    "# StandardScaler will standardize the dataset’s features onto unit scale (mean = 0 and variance = 1) \n",
    "\n",
    "x = iris_features.values\n",
    "scaled_array = StandardScaler().fit_transform(x) #This is an array of the standardized values of the four feature columns\n",
    "iris_standardized = pd.DataFrame(data= np.c_[scaled_array], \\\n",
    "                             columns = ('sepal length', 'sepal width', 'petal length', 'petal width'))\n",
    "\n",
    "# View standardized data frame\n",
    "iris_standardized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first decision in PCA is to select the number of components to reduce to.\n",
    "# The goal is to reduce dimensions while still retaining most of the variance of the features.\n",
    "\n",
    "# Start with components=4 (use all) to assess what % of the variance each principal component contains\n",
    "pca = PCA(n_components=4)\n",
    "principalComponents = pca.fit_transform(scaled_array)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scree Plot\n",
    "\n",
    "The scree plot helps to determine the optimal number of components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Explained variance as a function of Principal Components considered\n",
    "\n",
    "figure=plt.figure(figsize=(9,5))\n",
    "plt.plot(range(1,5),pca.explained_variance_ratio_.cumsum(),marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Scree Plot')\n",
    "plt.locator_params(axis='x', nbins=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the cumulative % of variance explained by the principal components, I can see that principal components 1, 2, and 3 contain 99.4% of the variation (information). We can also see on the Scree Plot above that the explained variance gain from 2 to 3 is signficant. As such 3 principal compnents will be used for the PCA data frame in this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to the scaled version of the Iris features, using 3 principal components (based on above analysis)\n",
    "# Create and display PCA Features data frame. \n",
    "# Note: there isn't particular meaning assigned to each principal component, the new components are just the two main \n",
    "# dimensions of variation - which are linear combinations of the original variables.\n",
    "\n",
    "iris_model = PCA(n_components=3)\n",
    "principalComponents = iris_model.fit_transform(scaled_array)\n",
    "iris_PCA = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal comp 1', 'principal comp 2', 'principal comp 3'])\n",
    "iris_PCA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of the iris data reduced to 3 Principal Components\n",
    "iris_PCA_target=iris_PCA.assign(target=iris_df['target'])\n",
    "fig = px.scatter_3d(iris_PCA_target, x='principal comp 1', y='principal comp 2', z='principal comp 3',\n",
    "              color='target', size_max=8, opacity = .7)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employ clustering techniques using defined clustering function: on original data frame and on PCA data frame\n",
    "* KMeans\n",
    "* Agglomerative\n",
    "* DBScan\n",
    "* Mean Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use optimal number of clusters derived from Elbow Plot = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clustering(KMeans, iris_features, iris_target, n_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization notes: Setosa is nicely grouped by itself and versicolor and virgnica are fairly separated but there are some versicolor grouped with virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clustering(KMeans, iris_PCA, iris_target, n_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization notes: Setosa is again nicely grouped by itself but there is much more overlap between versicolor and virginica than with the original data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "KMeans on the original Iris data frame performed quite well, with a V-measure of 0.90.  \n",
    "\n",
    "In this case, PCA appears to have diminished the results, with a V-measure of only 0.66."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use optimal number of clusters derived from Dendogram = 3\n",
    "\n",
    "The linkage criterion determines which distance to use between sets of observation.\n",
    "\n",
    "The option ‘ward’ minimizes the variance of the clusters being merged, euclidean is the distance used for this criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clustering(AgglomerativeClustering, iris_features, iris_target, n_clusters=3, affinity='euclidean', linkage='ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization notes: Setosa is nicely grouped by itself, and there is pretty good separation of versicolor and virginica, with just a little overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clustering(AgglomerativeClustering, iris_PCA, iris_target, n_clusters=3, affinity='euclidean', linkage='ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization notes: Setosa is clustered by itself with the exception of one point. Virginica is also clustered fairly well with one exception.  Versicolor though, has what looks like as many diamonds as squares which indicates this method/data frame combination did not do a good job of differentiating versicolor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "Agglomerative clustering on the original Iris data frame performed fairly well, with a V-measure of 0.86.  \n",
    "\n",
    "Again in this case, PCA appears to have diminished the results, with a V-measure of only 0.68."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBScan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on DBScan Inputs and Output\n",
    "\n",
    "* Input - eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other\n",
    "* Input min_samples: The number of samples (or total weight) in a neighborhood for a point to be considered as a core point\n",
    "* Output noise: points that don't have the min # of points within the eps distance (not core points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clustering(DBSCAN, iris_features, iris_target, eps = 0.2, min_samples = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization notes: Setosa is nicely separated as it's own cluster, but versicolor and virginica are combined as one cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clustering(DBSCAN, iris_PCA, iris_target, eps = 0.48, min_samples = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization notes: Setosa is nicely separated as it's own cluster, but versicolor and virginica are combined as one cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "DBScan on the original Iris data frame did not perform very well: it only detected 2 clusters and had a V-measure of 0.73.  \n",
    "\n",
    "In this case, PCA appears to have not diminished nor improved the results, as the results for the PCA data frame were the same as the results for the original Iris data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean shift clustering aims to discover “blobs” in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.\n",
    "\n",
    "n_jobs is the number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. -1 means using all processors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clustering(MeanShift, iris_features, iris_target, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization notes: Setosa is nicely separated as it's own cluster, but versicolor and virginica are combined as one cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clustering(MeanShift, iris_PCA, iris_target, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization notes: Setosa is all clustered together but this cluster also contains some versicolor.  The remaining versicolor are combined in one cluster with virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "Mean Shift on the original Iris data frame did not perform very well: it only detected 2 clusters and had a V-measure of 0.73. Note that these results are the same as the DBScan on the original Iris data frame. \n",
    "\n",
    "In this case, PCA appears to have diminished the results.  It too, detected only 2 clusters but it's V-measure of 0.61 reflects the fact that six of the Versicolor are not in the same cluster as the other Versicolor records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the various techniques, on both the original Iris data frame and on the corresponding PCA data frame, we will use the V-measure.  This measure combines the concept of completeness (all members of a target class are in the same cluster) and homogeneity (each cluster contains only members of one target class).\n",
    "\n",
    "The results of our analysis are:\n",
    "\n",
    "Method         | Data Frame | V-Measure\n",
    ":-----         | :----      | :-----\n",
    "KMeans         | Iris       | 0.90\n",
    "Kmeans         | PCA        | 0.66\n",
    "Agglomerative  | Iris       | 0.86\n",
    "Agglomerative  | PCA        | 0.68\n",
    "DBScan         | Iris       | 0.73\n",
    "DBScan         | PCA        | 0.73\n",
    "MeanShift      | Iris       | 0.73\n",
    "MeanShift      | PCA        | 0.61\n",
    "\n",
    "For the clustering techniques and data set used in this analysis:\n",
    "\n",
    "* The technique that performed the best was Kmeans on the original Iris data frame.\n",
    "* The technique that performed the worst was MeanShift on the PCA data frame.\n",
    "\n",
    "Also, for this data set and using 3 principal components in our PCA data frame, PCA appears to be at best neutral (in DBScan) and at worst to have a detrimental effect on our clustering results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
