{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Dataset Clustering\n",
    "\n",
    "## Employ multiple clustering techniques on the Iris dataset, with and without PCA.  Evaluate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans, MeanShift\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Function for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_clustering(func, data, target, **args):\n",
    "        \n",
    "        \"\"\"This function generates a clustering model using a scikit-learn function, displays a confusion matrix and plot of\n",
    "        the data (using the first 3 columns of the data), and returns the model\n",
    "            \n",
    "            Arguments:\n",
    "            func - A scikit-learn clustering function\n",
    "            data - A pandas dataframe containing the features to be used in clustering\n",
    "            target - The target values for comparison with the model predictions\n",
    "            \n",
    "            Any keyword arguments requried for the specific clustering function should also be included at the end of the call\n",
    "        \"\"\"\n",
    "        \n",
    "        normalized = normalize(data)\n",
    "        \n",
    "        model = func(**args)\n",
    "        preds = model.fit_predict(normalized)\n",
    "        \n",
    "        preds_df = data.copy()\n",
    "        preds_df['target'] = target\n",
    "        preds_df['preds'] = preds\n",
    "        \n",
    "        conf = pd.crosstab(preds_df['preds'], preds_df['target'], margins=True)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        display(conf)\n",
    "        \n",
    "        homogeneity = metrics.homogeneity_score(preds_df[\"target\"], preds_df[\"preds\"])\n",
    "        completeness = metrics.completeness_score(preds_df[\"target\"], preds_df[\"preds\"])\n",
    "        v_measure = metrics.v_measure_score(preds_df[\"target\"], preds_df[\"preds\"], beta=1.0)\n",
    "\n",
    "        print(f\"Homogeneity_score: {homogeneity:.2f}, measures that each cluster contains only members of a single class\")\n",
    "        print(f\"Completeness_score: {completeness:.2f}, easures that all members of a given class are assigned to the same cluster\")\n",
    "        print(f\"V-measure: {v_measure:.2f}, harmonic mean of homogeneity and completeness\")\n",
    "\n",
    "        scatter = px.scatter_3d(preds_df, x=preds_df.columns[0], y=preds_df.columns[1], z=preds_df.columns[2],\n",
    "                            color = 'target', symbol = 'preds')\n",
    "        scatter.show()\n",
    "        \n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset, Explore and Display Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "iris_df['target'] = iris_df['target'].replace([0,1,2],['setosa', 'versicolor', 'virginica'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 3 unique target variables: setosa, versicolor and virginica; each a species of Iris\n",
    "* Changed the target attribute labels to a descriptive string vs numerical category for ease in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The dataset contains 150 observations, has 4 predictive attributes and 1 target variable\n",
    "* The 4 predictive attributes are numerical, the target variable is categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* petal length has the largest range and greatest variation of the 4 attributes, and also has the greatest difference of the 4 between its mean and median\n",
    "* for the other three attributes, their mean approximates their median which suggests the mean is not affected by outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(iris_df)\n",
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "* The dataset has zero missing observations\n",
    "* This is a balanced dataset in that each of the three target labels have the same number of observations\n",
    "* The distributions of sepal length and sepal width are fairly normal\n",
    "* The distributions of petal length and petal width both have two distinct groupings\n",
    "* Correlation - because the 4 predictive attributes are all numerical, refer to the Pearson's r chart, above:\n",
    "    * Sepal width and sepal length appear to be uncorrelated\n",
    "    * Petal width and petal length appear to be highly correlated \n",
    "    * Petal length and sepal length appear to be fairly correlated\n",
    "    * Petal width and sepal length also appear to be correlated, though less so than petal length and sepal length    \n",
    "* Correlation - see pair plot graphs below for visual confirmation of the above correlation observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Elbow Plot to determine optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Iris data frame without the target column\n",
    "\n",
    "iris_features = iris_df.drop(columns='target')\n",
    "iris_target = iris_df['target']\n",
    "iris_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Elbow plot of inertia values to determine optimal number of clusters to use in a K-Means clustering method\n",
    "# inertia is the sum of the squared distances of observations to their closest cluster center\n",
    "\n",
    "inertia_values = []\n",
    "cluster_centers = []\n",
    "K = range(1,11) #Try number of clusters from 1 to 10\n",
    "\n",
    "for k in K:\n",
    "    k_mean_model = KMeans(n_clusters = k)\n",
    "    k_mean_model.fit(iris_features)\n",
    "    inertia_values.append(k_mean_model.inertia_) #track the inertia values for each number of clusters\n",
    "    cluster_centers.append(k_mean_model.cluster_centers_) #track the cluster centers for each number of clusters\n",
    "\n",
    "# Create data frame of values for elbow plot\n",
    "elbow_data = {'Number of Clusters': K, 'Inertia': inertia_values}\n",
    "elbow_df = pd.DataFrame(elbow_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the Elbow plot\n",
    "\n",
    "fig_dims = (6, 3)\n",
    "fig, ax = plt.subplots(figsize=fig_dims)\n",
    "\n",
    "sns.set_theme(style = \"whitegrid\")\n",
    "sns.pointplot(data = elbow_df, x = 'Number of Clusters' ,y = 'Inertia', markers=[\"o\"])\\\n",
    ".set(title='Eblow Plot using Inertia for K-Means Clustering of Iris Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of clusters is at the \"elbow\" of the graph, where the Inertia begins to decline in a linear fashion.  In this case the optimal number is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display cluster centers for k = 3 clusters in k-means model\n",
    "cluster_centers[2:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce data frame using Principal Components Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA is affected by scale, so we need to scale our features before applying PCA\n",
    "# StandardScaler will standardize the dataset’s features onto unit scale (mean = 0 and variance = 1) \n",
    "\n",
    "x = iris_features.values\n",
    "scaled_array = StandardScaler().fit_transform(x) #This is an array of the standardized values of the four feature columns\n",
    "iris_standardized = pd.DataFrame(data= np.c_[scaled_array], \\\n",
    "                             columns = ('sepal length', 'sepal width', 'petal length', 'petal width'))\n",
    "\n",
    "# View standardized data frame\n",
    "iris_standardized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first decision in PCA is to select the number of components to reduce to.\n",
    "# The goal is to reduce dimensions while still retaining most of the variance of the features.\n",
    "\n",
    "# Start with components=4 (use all) to assess what % of the variance each principal component contains\n",
    "pca = PCA(n_components=4)\n",
    "principalComponents = pca.fit_transform(scaled_array)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Explained variance as a function of Principal Components considered\n",
    "\n",
    "figure=plt.figure(figsize=(9,5))\n",
    "plt.plot(range(1,5),pca.explained_variance_ratio_.cumsum(),marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Scree Plot')\n",
    "plt.locator_params(axis='x', nbins=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the cumulative % of variance explained by the principal components, I can see that principal components 1, 2, and 3 contain 99.4% of the variation (information). We can also see on the Scree Plot above that the explained variance gain from 2 to 3 is signficant. As such 3 principal compnents will be used for the PCA data frame in this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to the scaled version of the Iris features, using 3 principal components (based on above analysis)\n",
    "# Create and display PCA Features data frame. \n",
    "# Note: there isn't particular meaning assigned to each principal component, the new components are just the two main \n",
    "# dimensions of variation - which are linear combinations of the original variables.\n",
    "\n",
    "iris_model = PCA(n_components=3)\n",
    "principalComponents = iris_model.fit_transform(scaled_array)\n",
    "iris_PCA = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n",
    "iris_PCA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of the iris data reduced to 3 Principal Components\n",
    "iris_PCA_target=iris_PCA.assign(target=iris_df['target'])\n",
    "fig = px.scatter_3d(iris_PCA_target, x='principal component 1', y='principal component 2', z='principal component 3',\n",
    "              color='target', size_max=8, opacity = .7)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employ Clustering Techniques - on original data frame and on PCA data frame\n",
    "* Partitioning Method: K-Means\n",
    "* Hierarchical Method: sklearn's AgglomerativeClustering\n",
    "* Density-based Method: DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning Method: K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means on original Iris data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_features = iris_df.drop(columns='target')\n",
    "iris_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run KMeans on the original features of the Iris data frame, using 3 clusters\n",
    "\n",
    "k_mean_model = KMeans(n_clusters = 3)\n",
    "k_mean_model.fit(iris_features)\n",
    "\n",
    "# Create new Datafram with predicted cluster from KMeans and target label from iris_df\n",
    "iris_kmeans=iris_features.assign(k_means=k_mean_model.predict(iris_features),\n",
    "                                 target=iris_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display modified iris_features data frame\n",
    "iris_kmeans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table to assess clustering accuracy\n",
    "\n",
    "my_crosstab = pd.crosstab(index=iris_kmeans[\"k_means\"],columns=iris_kmeans[\"target\"], margins=True) \n",
    "my_crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table indicates an purity score of 89.3% ((36+50+48)/150 = .89333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering performance evaluation - additional measures\n",
    "\n",
    "# Rand Index - Adjusted: a function that measures the similarity of the two assignments, adjusted corrects for chance\n",
    "print('Adjusted Rand Index:', metrics.adjusted_rand_score(iris_kmeans[\"target\"], iris_kmeans[\"k_means\"]),\\\n",
    "     \", measures the similarity of the two assignments: true label and predicted cluster\",'\\n')\n",
    "#homogeneity_score: each cluster contains only members of a single class.\n",
    "print('Homogeneity_score:',metrics.homogeneity_score(iris_kmeans[\"target\"], iris_kmeans[\"k_means\"]),\\\n",
    "     \", measures that each cluster contains only members of a single class\",'\\n')\n",
    "#completeness_score: all members of a given class are assigned to the same cluster.\n",
    "print('Completeness_score:', metrics.completeness_score(iris_kmeans[\"target\"], iris_kmeans[\"k_means\"]),\\\n",
    "     \", measures that all members of a given class are assigned to the same cluster\",'\\n')\n",
    "#v_measure_score: harmonic mean of homogeneity and completeness).\n",
    "print('V-measure:', metrics.v_measure_score(iris_kmeans[\"target\"], iris_kmeans[\"k_means\"], beta=1.0),\\\n",
    "     \", harmonic mean of homogeneity and completeness, beta>1 gives more weight to homogeneity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph actual labels vs. clustering for sepal length/sepal width and petal length/petal width\n",
    "\n",
    "fig, ((ax1, ax2)) = plt.subplots(2, 1, figsize = (15, 20))\n",
    "sns.scatterplot(x='sepal length (cm)', y='sepal width (cm)', data = iris_kmeans, hue='k_means',\n",
    "                style='target',palette=\"deep\",ax=ax1, s=100)\\\n",
    ".set(title='Sepal Length vs. Width:     Color=K-Means, Shape=Actual Target') \n",
    "sns.scatterplot(x='petal length (cm)', y='petal width (cm)', data = iris_kmeans, hue='k_means',\n",
    "                style='target',palette=\"deep\",ax=ax2, s=100)\\\n",
    ".set(title='Petal Length vs. Width:     Color=K-Means, Shape=Actual Target');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K=Means on PCA Features data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall the structure of the PCA Features data frame\n",
    "\n",
    "iris_PCA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run KMeans on the PCA Features data frame, using 3 clusters\n",
    "\n",
    "k_mean_model = KMeans(n_clusters = 3)\n",
    "k_mean_model.fit(iris_PCA)\n",
    "\n",
    "# Add column to iris_PCA_df dataframe: predicted cluster from KMeans\n",
    "iris_PCA_kmeans = iris_PCA.assign(k_means=k_mean_model.predict(iris_PCA),\n",
    "                                  target=iris_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display modified iris_PCA_df data frame\n",
    "iris_PCA_kmeans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table to assess clustering accuracy\n",
    "\n",
    "my_crosstab = pd.crosstab(index=iris_PCA_kmeans[\"k_means\"],columns=iris_PCA_kmeans[\"target\"], margins=True) \n",
    "my_crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table indicates an purity score of 83.3% ((36+50+39)/150 = .8333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering performance evaluation - additional measures\n",
    "\n",
    "# Rand Index - Adjusted: a function that measures the similarity of the two assignments, adjusted corrects for chance\n",
    "print('Adjusted Rand Index: ', metrics.adjusted_rand_score(iris_PCA_kmeans[\"target\"], iris_PCA_kmeans[\"k_means\"]),\\\n",
    "     \", measure the similarity of the two assignments: true label and predicted cluster\",'\\n')\n",
    "#homogeneity_score: each cluster contains only members of a single class.\n",
    "print('Homogeneity_score: ',metrics.homogeneity_score(iris_PCA_kmeans[\"target\"], iris_PCA_kmeans[\"k_means\"]),\\\n",
    "     \", measures that each cluster contains only members of a single class\",'\\n')\n",
    "#completeness_score: all members of a given class are assigned to the same cluster.\n",
    "print('Completeness_score: ', metrics.completeness_score(iris_PCA_kmeans[\"target\"], iris_PCA_kmeans[\"k_means\"]),\\\n",
    "     \", measures that all members of a given class are assigned to the same cluster\",'\\n')\n",
    "#v_measure_score: harmonic mean of homogeneity and completeness).\n",
    "print('V-measure: ', metrics.v_measure_score(iris_PCA_kmeans[\"target\"], iris_PCA_kmeans[\"k_means\"], beta=1.0),\\\n",
    "     \", harmonic mean of homogeneity and completeness, beta>1 gives more weight to homogeneity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph actual labels vs. clustering for sepal length/sepal width and petal length/petal width\n",
    "\n",
    "fig, ((ax1)) = plt.subplots(1, figsize = (15, 8))\n",
    "sns.scatterplot(x='principal component 1', y='principal component 2', data = iris_PCA_kmeans,\n",
    "                hue='k_means',style='target', palette=\"deep\",s=100, ax=ax1)\\\n",
    ".set(title='Principal Component 1 vs. Principal Component 2: Color=K-Means, Shape/Size=Actual Target');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Method: sklearn's AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_features[iris_features.columns[:-2]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_normalized = pd.DataFrame(normalize(iris_features),columns=iris_features.columns[:])\n",
    "iris_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Like an elbow plot for k-means, a dendogram helps determine the ideal number of clusters for hierarchical clustering\n",
    "# Note: although this chart shows 2 clusters at the second level, we'll go down to the 3rd level and use 3 clusters\n",
    "\n",
    "plt.figure(figsize=(18, 7))  \n",
    "plt.title(\"Dendrograms\")  \n",
    "dend = shc.dendrogram(shc.linkage(iris_normalized, method='ward'))\n",
    "plt.axhline(y=0.5, color='r', linestyle='--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the linkage criterion determines which distance to use between sets of observation.\n",
    "# ‘ward’ minimizes the variance of the clusters being merged, euclidean is the distance used for this criterion\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')  \n",
    "cluster.fit_predict(iris_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new data frame using Agglomerative Clustering\n",
    "iris_agglo=iris_normalized.assign(hierarchical=cluster.fit_predict(iris_normalized),\n",
    "                                 target=iris_df['target'])\n",
    "iris_agglo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table to assess clustering accuracy\n",
    "\n",
    "my_crosstab = pd.crosstab(index=iris_agglo['hierarchical'],columns=iris_agglo[\"target\"], margins=True) \n",
    "my_crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table indicates an purity score of 96.0% ((48+50+46)/150 = 0.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering performance evaluation - additional measures\n",
    "\n",
    "# Rand Index - Adjusted: a function that measures the similarity of the two assignments, adjusted corrects for chance\n",
    "print('Adjusted Rand Index: ', metrics.adjusted_rand_score(iris_agglo[\"target\"], iris_agglo['hierarchical']),\\\n",
    "     \", measure the similarity of the two assignments: true label and predicted cluster\",'\\n')\n",
    "#homogeneity_score: each cluster contains only members of a single class.\n",
    "print('Homogeneity_score: ',metrics.homogeneity_score(iris_agglo[\"target\"], iris_agglo['hierarchical']),\\\n",
    "     \", measures that each cluster contains only members of a single class\",'\\n')\n",
    "#completeness_score: all members of a given class are assigned to the same cluster.\n",
    "print('Completeness_score: ', metrics.completeness_score(iris_agglo[\"target\"], iris_agglo['hierarchical']),\\\n",
    "     \", measures that all members of a given class are assigned to the same cluster\",'\\n')\n",
    "#v_measure_score: harmonic mean of homogeneity and completeness).\n",
    "print('V-measure: ', metrics.v_measure_score(iris_agglo[\"target\"], iris_agglo['hierarchical'], beta=1.0),\\\n",
    "     \", harmonic mean of homogeneity and completeness, beta>1 gives more weight to homogeneity\",'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph actual labels vs. clustering for sepal length/sepal width and petal length/petal width\n",
    "\n",
    "fig, ((ax1, ax2)) = plt.subplots(2, 1, figsize = (15, 15))\n",
    "sns.scatterplot(x='sepal length (cm)', y='sepal width (cm)', data = iris_agglo, hue='hierarchical',style='target',size='target', palette=\"deep\",ax=ax1)\\\n",
    ".set(title='Sepal Length vs. Width: Color=hierarchical cluster, Shape/Size=Actual Target')\n",
    "sns.scatterplot(x='petal length (cm)', y='petal width (cm)', data = iris_agglo, hue='hierarchical',style='target',size='target',palette=\"deep\",ax=ax2)\\\n",
    ".set(title='Petal Length vs. Width: Color=hierarchical cluster, Shape/Size=Actual Target');# Scatterplot of the iris data reduced to 3 Principal Components\n",
    "\n",
    "# Creating 3D axes\n",
    "# fig = plt.figure(figsize=(6,6))\n",
    "# ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "# fig.add_axes(ax)\n",
    "\n",
    "# cmap = ListedColormap(sns.color_palette(\"husl\", 256).as_hex()) # Colormap magic\n",
    "\n",
    "# scatter_3d = ax.scatter(pca_3_df['PC1'], pca_3_df['PC2'], pca_3_df['PC3'], c=colors,\n",
    "#                          marker='o', cmap=cmap, alpha=1)\n",
    "\n",
    "# ax.set_xlabel('PC1')\n",
    "# ax.set_ylabel('PC2')\n",
    "# ax.set_zlabel('PC3')\n",
    "# legend = ax.legend(*scatter_3d.legend_elements(), title='Class')\n",
    "# ax.add_artist(legend)\n",
    "# plt.show()\n",
    "\n",
    "fig2 = px.scatter_3d(x=iris_PCA['principal component 1'], y=iris_PCA['principal component 2'], \n",
    "                     z=iris_PCA['principal component 3'], color=iris_df['target'])\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density-based Method: DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn implementation provides a default for the eps and min_samples parameters, but you’re generally expected to tune those. The eps parameter is the maximum distance between two data points to be considered in the same neighborhood. The min_samples parameter is the minimum amount of data points in a neighborhood to be considered a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = iris_features.copy()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check through this example - need to standardize? normalize? Look up diff between the two and when they should be used.\n",
    "# Do for original df and for pca separately\n",
    "# eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other\n",
    "# min_samples: The number of samples (or total weight) in a neighborhood for a point to be considered as a core point\n",
    "# noise: points that don't have the min # of points within the eps distance (not core points)\n",
    "dbscan=DBSCAN(eps = 0.45, min_samples = 5)\n",
    "dbscan.fit(x)\n",
    "pca=PCA(n_components=2).fit(x)\n",
    "pca_2d=pca.transform(x)\n",
    "\n",
    "for i in range(0, pca_2d.shape[0]):\n",
    "    if dbscan.labels_[i] == 0:\n",
    "        c1 = plt.scatter(pca_2d[i, 0], pca_2d[i, 1], c='r', marker='+')\n",
    "    elif dbscan.labels_[i] == 1:\n",
    "        c2 = plt.scatter(pca_2d[i, 0], pca_2d[i, 1], c='g', marker='o')\n",
    "    elif dbscan.labels_[i] == -1:\n",
    "        c3 = plt.scatter(pca_2d[i, 0], pca_2d[i, 1], c='b', marker='*')\n",
    "        \n",
    "plt.legend([c1, c2, c3], ['Cluster 1', 'Cluster 2', 'Noise'])\n",
    "plt.title('DBSCAN finds 2 clusters and Noise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check through this example - need to standardize? normalize? Look up diff between the two and when they should be used.\n",
    "# Do for original df and for pca separately\n",
    "# eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other\n",
    "# min_samples: The number of samples (or total weight) in a neighborhood for a point to be considered as a core point\n",
    "# noise: points that don't have the min # of points within the eps distance (not core points)\n",
    "dbscan=DBSCAN(eps = 0.45, min_samples = 5)\n",
    "dbscan.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns to iris_features dataframe: predicted cluster from KMeans and target label from iris_df\n",
    "x['DBSCAN_cluster']=dbscan.labels_\n",
    "x['target']=iris_df['target']\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table to assess clustering accuracy\n",
    "\n",
    "my_crosstab = pd.crosstab(index=x['DBSCAN_cluster'],columns=x['target'], margins=True) \n",
    "my_crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table indicates an purity score of 84.0% ((48+43+35)/150 = 0.84)- NOT SURE THIS CALC APPLIES HERE B/C it only produced two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering performance evaluation - additional measures\n",
    "\n",
    "# Rand Index - Adjusted: a function that measures the similarity of the two assignments, adjusted corrects for chance\n",
    "print('Adjusted Rand Index: ', metrics.adjusted_rand_score(x[\"target\"], x['DBSCAN_cluster']),\\\n",
    "     \", measure the similarity of the two assignments: true label and predicted cluster\")\n",
    "#homogeneity_score: each cluster contains only members of a single class.\n",
    "print('Homogeneity_score: ',metrics.homogeneity_score(x[\"target\"], x['DBSCAN_cluster']),\\\n",
    "     \", measures that each cluster contains only members of a single class\")\n",
    "#completeness_score: all members of a given class are assigned to the same cluster.\n",
    "print('Completeness_score: ', metrics.completeness_score(x[\"target\"], x['DBSCAN_cluster']),\\\n",
    "     \", measures that all members of a given class are assigned to the same cluster\")\n",
    "#v_measure_score: harmonic mean of homogeneity and completeness).\n",
    "print('V-measure: ', metrics.v_measure_score(x[\"target\"], x['DBSCAN_cluster'], beta=1.0),\\\n",
    "     \", harmonic mean of homogeneity and completeness, beta>1 gives more weight to homogeneity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph actual labels vs. clustering for sepal length/sepal width and petal length/petal width\n",
    "\n",
    "fig, ((ax1, ax2)) = plt.subplots(2, 1, figsize = (15, 20))\n",
    "sns.scatterplot(x='sepal length (cm)', y='sepal width (cm)', data = x, hue='DBSCAN_cluster',\n",
    "                style='target',s=100, palette=\"deep\",ax=ax1)\\\n",
    ".set(title='Sepal Length vs. Width: Color=DBSCAN_cluster, Shape=Actual Target')\n",
    "sns.scatterplot(x='petal length (cm)', y='petal width (cm)', data = x, hue='DBSCAN_cluster',\n",
    "                style='target',s=100,palette=\"deep\",ax=ax2)\\\n",
    ".set(title='Petal Length vs. Width: Color=DBSCAN_cluster, Shape/Size=Actual Target');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Shift Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Mean shift clustering on the orgininal iris \n",
    "# data using an estimated bandwidth\n",
    "\n",
    "mean_clusters = MeanShift(n_jobs=-1)\n",
    "mean_clusters.fit(iris_features)\n",
    "\n",
    "labels = mean_clusters.labels_\n",
    "iris_meanshift=iris_features.assign(meanshift=labels,\n",
    "                                 target=iris_df['target'])\n",
    "iris_meanshift.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_crosstab = pd.crosstab(index=iris_meanshift[\"meanshift\"],columns=iris_meanshift[\"target\"], margins=True) \n",
    "my_crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanshift clustering fails to determine there are 3 clusters for the original Iris data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Mean shift clustering on the PCA iris data using an estimated bandwidth\n",
    "\n",
    "mean_clusters = MeanShift(n_jobs=-1)\n",
    "mean_clusters.fit(iris_PCA)\n",
    "\n",
    "labels2 = mean_clusters.labels_\n",
    "iris_meanshift_PCA=iris_PCA.assign(meanshift=labels2,\n",
    "                                 target=iris_df['target'])\n",
    "iris_meanshift_PCA.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_crosstab = pd.crosstab(index=iris_meanshift_PCA[\"meanshift\"],columns=iris_meanshift_PCA[\"target\"], margins=True) \n",
    "my_crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanshift clustering fails to determine there are 3 clusters for the PCA Iris data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation with clustering function (I didn't want to delete any previous work yet, but I think doing it this way could help clean things up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clustering(KMeans, iris_features, iris_target, n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clustering(KMeans, iris_PCA, iris_target, n_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clustering(AgglomerativeClustering, iris_features, iris_target, n_clusters=3, affinity='euclidean', linkage='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clustering(AgglomerativeClustering, iris_PCA, iris_target, n_clusters=3, affinity='euclidean', linkage='ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clustering(DBSCAN, iris_features, iris_target, eps = 0.2, min_samples = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clustering(DBSCAN, iris_PCA, iris_target, eps = 0.48, min_samples = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clustering(MeanShift, iris_features, iris_target, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clustering(MeanShift, iris_PCA, iris_target, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
